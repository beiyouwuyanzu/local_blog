---
title: Sparse Transformers
date: 2023-09-04 23:46:59
---

> https://arxiv.org/pdf/1904.10509.pdf
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202308221100270.png)

![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202308221100710.png)

GPT-3与GPT-2采用相同的模型和架构，包含初始化、预归一化和可逆分词等部分。不同之处在于GPT-3的transformer的层中采用了交替的密集和局部带状稀疏注意力（提升计算效率）模式，类似于之前提出的稀疏变换器。稀疏变换器指的是通过稀疏化Transformer模型中的attention矩阵来达到减少内存消耗、降低计算力的方法，其整体思想是将attention操作分为两个部分进行，将计算复杂度从O(n^2) 降低到O(nlogn) 

。
传统Transformer对于序列中的第i个元素（记为Si），它的自注意力定义为$S_i=j:j≤i$，即在生成第i个元素时，它可以关注所有在它之前的位置，包括前面已经生成的元素以及自身位置。其使得每个元素都能够考虑所有之前的位置，以便在生成当前元素时能够充分利用前面的上下文信息，保持了序列的自相关性。分解自注意力则具有p个单独的注意头，其中第m个注意头定义了索引的子集$A_i^m⊂j:j≤i$
,且$S_i=A_i^{(m)}$,同时每个子集大小保持与$^p\sqrt{n}$成正比。
标准的密集注意力仅仅执行了对
$Attend(X,S)=(a(x_i,S_i))_{i∈{1,2,...,n}}$


中定义的关注函数的线性变换：
$attention(X)=W_p⋅attend(X,S)$其中，$W_p$表示后注意权重矩阵。
集成分解自注意力的最简单技术是每个残差块使用一种注意机制类型，并以顺序或按超参数确定的比率交错地排列它们：
$attention(X)=W_p⋅attend(X,A( 
r \ mod\  p))$
﻿其中，r是当前残差块的索引，p是分解注意头的数量。（此外还有两种，感兴趣的同学可以进一步阅读原文Sparse Transformer：﻿《Generating Long Sequences with Sparse Transformers》﻿）