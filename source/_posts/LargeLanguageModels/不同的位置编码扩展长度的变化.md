---
title: 不同的位置编码扩展长度的变化
date: 2023-09-05 23:19:21
---


在Meta和合作机构于21年发表的论文「TRAIN SHORT, TEST LONG」[2]中，作者展示了采用这一思路的评估结论。它比较了Sinusoidal，Rotary Position和T5 Bias三种位置embedding算法在固定长度训练，变长的序列输入做预测时，perplexity指标[3]变化的情况。perplexity也叫困惑度指标，用于评估LLM的生成能力。困惑度越低，模型生成的文本越符合语言的统计规律，越可能是人类写的文本。反之则越不符合人类习惯，越像胡说八道。

![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202308171916023.png)

上面左图是当模型按照512 token长度序列训练后，测试512-16000个token长度的输入序列预测后的perplexity指标变化情况。右图和左图类似，只是模型是按照1024 token长度训练。可以明显看出，Sinusoidal，Rotary Position和T5 Bias三种算法的perplexity随着预测序列长度的增加而迅速上涨，说明此时大模型输出的文本已经不可用，完全无意义了。


## AliBi
ALiBi方法ALiBi方法的思想是采用外延的方式。去掉position embedding叠加这一步骤，改为在query和key向量点乘之后，增加一个bias：
$softmax(q_iK^T+m \cdot [-(i-1),\dots,-2, -1, 0])$其中i为token的位置偏移量($1\le i \le L$)，m为每个head分配的倾斜常量，当head为n的时候，m为几何数列：$\frac{1}{2^{\frac{8}{n}}}, \frac{1}{2^{\frac{16}{n}}},\dots, \frac{1}{2^8}$ 。
通过增加这样一个bias，模型在训练时将学习到位置关系，起到了position embedding的效果。可视化效果如下：
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202308171919782.png)
其实就是在softmax之前减掉了一个非负矩阵，可以看做：
$q_ik_j-m \cdot |i-j|$类似减去这样一个矩阵：
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202308171920684.png)
从上图可以看出，本质上是将token之间的注意力权重，按照距离进行了线性降权，两个token之间的距离越远，降权越多。距离越近，降权越低。这样当预测时序列长度距离远超过训练训练长度时，整个注意力集中在局部区域内，相当于一种局部attention机制了。
ALiBi论文认为，这种方法相对经典的Sinusoidal方法延展性性更好，预测超过训练长度的上下文长度，也能给出较低的perplexity值，下图也给出了测试数据。
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202308171920796.png)
实际上，BLOOM[7], MPT[8]，baichuan-13B模型[9]采用的就是ALiBi位置编码，据baichuan官方文档介绍，采用ALiBi位置编码后，计算量更小，对推理性能有显著提升；与标准的 LLaMA-13B 相比，平均推理速度 (tokens/s) 实测提升 31.6%（但没给出perplexity评估数据）。

## 线性内插法
线性内插法从2021年到2023年期间，业界也出现了很多类似做外推方法的扩展，典型的如Microsoft公司发表的XPOS方法[10]等。但这些方法一直通过外延的思路来解决问题，效果上并没有特别大的突破，尤其是如何在流行的RoPE位置编码算法的基础上进行扩展，并没有特别好的方法。直到2023年6月，由网友kaiokendev在他的博客上发表了一篇博文，只需在RoPE编码算法的基础之上增加2行代码，就能搞定上下文长度扩展问题。据作者说明，他花了1个月的时间研究各种文献，最终从AliBi作者的一次演讲中获得灵感，发现只需要2行代码就能搞定。这种方法最大可以将LLaMa这样的模型支持的上下文长度稳定地从2K扩展到8K，可谓神来之笔。
后来我们可以看出，这种方法是一种线性内插法。介绍这种方法需要对RoPE位置编码算法[11]有一些了解。RoPE是由苏剑林等人发明的一种旋转式位置编码算法，被用于LLaMa等一众开源LLM中。RoPE的基本原理很简单，给定一个位置序号$m \in [1,c)$和一个嵌入向量 $x:=[x_1,x_2,\dots,x_d]^T$，其中d为每个attention head的维度。RoPE算法定义如下矩阵变换：
$f_{q,k}(x_m,m) = R^d_{\Theta,m}W_{q,k}x_m$其中:Ï
$R^d_{\Theta,m} = \begin{pmatrix} cos(m\theta_1) & -sin(m\theta_1) & 0 & 0 & \dots & 0 & 0 \\ sin(m\theta_1) & cos(m\theta_1) & 0 & 0 & \dots & 0 & 0 \\ 0 & 0 & cos(m\theta_2) & -sin(m\theta_2) & \dots & 0 & 0 \\ 0 & 0 & sin(m\theta_2) & cos(m\theta_2) & \dots & 0 & 0 \\ \vdots & \vdots &\vdots &\vdots &\ddots &\vdots &\vdots & \\ 0 & 0& 0 & 0 & \dots & cos(m\theta_{d/2}) & -sin(m\theta_{d/2}) \\ 0 & 0 & 0 & 0 & \dots & sin(m\theta_{d/2}) & cos(m\theta_{d/2}) \\ \end{pmatrix}$其中$\theta_i=10000^{-2(i-1)/d},i \in [1,2,\dots,d/2]$。
通过这个函数变换query和key向量，就能在这两个向量中叠加了位置信息。这种编码相对Transformer算法经典的Sinusoidal方法而言，采用了相对位置编码，不受绝对位置影响。可以证明两个token之间的attention score仅依赖于两个token之间的距离。实际应用中，将每一层attention算子的query和key向量进行RoPE函数变换即可。
RoPE作为国产原创算法，一经提出，被业界迅速采纳。PaLM，LLaMA，GLM-130B都通过这个位置编码获得稳定性能提升。这个算法的实现简洁，效率较高，兼容性强。
但RoPE用来外推更长序列的时候遇到了困难。实际测试发现，如果用2048个token来训练模型，但在超过2048个token的位置预测输出回答问题，那么回答的问题将是乱七八糟，完全不会考虑前面2048个token的上下文。更有甚者，如果我们在第3000个token的位置回答问题，模型将无法利用2900位置的上下文信息。也就是说，上下文关联性完全丧失。
为什么会发生这个现象，如下图可看出端倪，如果我们将超过训练上下文长度的attention score随着长度变化展示出来，可以发现一旦超过训练长度，attention score将远超过正常值(例如$&gt;10^3$倍)，完全不可用，输出无意义的序列也是可想而知的。
![ae68dadff429d50494acc5d698f0bcf2.png](en-resource://database/3160:1)
既然外推可不行，是否还有其他方法？网友kaiokendev[4]和Meta的田渊栋团队[5]几乎同时发现了线性内插法。内插法顾名思义，不再是预测出超过训练上下文长度的位置向量值，而是将现有的位置向量进行缩放，使之支持更长的长度，如下图所示：
![f193fd7aeed91b0caeb21bd4b5161509.png](en-resource://database/3161:1)
而线性内插法的思想非常简单，就是将上面RoPE算法中的m替换成$\dfrac{mL}{L'}$即可，其中$L$为训练最大长度，$L'$为预测最大长度。举个例子，如果训练长度是2048，预测时支持4096长度，只需要将位置序号$[1, 2, \dots, 2048,\dots, 4096]$替换成$[0.5, 1, 1.5, \dots, 2048]$即可。而kaiokendev修改的两行代码，其实就是将变量$\theta_i$乘以一个scale变量即可。
但如果只是预测时变换了位置序号直接计算，效果并不好，这个方法还需要配合fine-tuning。fine-tuning过程需要按照期望的扩展上下文长度，用新的语料做Supervised Fine-tuning，按照缩放后的位置向量进行fine-tuning。按照论文[5]的介绍，只需要fine-tuning 1000步，即可获得理想的效果。在一些数据集中，上下文长度扩展到32K效果还保持不衰减。



## 动态内插法
那有没有什么更好的办法，不需要fine-tune就能直接在预测时扩充上下文长度呢？Reddit网友@bloc97在线性内插法提出一周后，就提出了新的NTK-Aware Scaled RoPE算法[6]。该算法无需fine-tune，轻松将2K上下文长度的LLama模型扩充到8K，而且perplexity值相对线性内插法更优，如下图所示：
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202308171940242.png)
动态内插法只是将RoPE公式中的$\theta_i$的计算改造成$\theta_i=(10000\alpha^{d/(d-2)})^{-2(i-1)/d}$，python实现也就三行代码。如此轻量级的改动就能实现无需fine-tuning还能扩充预测上下文长度，实在是令人惊奇。按照苏剑林的解释[12]，RoPE算法本质是数字n的$\beta$进制编码，动态内插法本质上是一种进制转换，通过换成更高进制的数字表示，可以在缩放范围的同时，提供更高的精度，从而实现更好的位置embedding表示。
在NTK-Aware Scaled RoPE算法基础上，reddit网友@emozilla提出了Dynamically NTK-Aware Scaled RoPE算法[15]，实际上将算法中的$\alpha$参数进一步动态缩放。按照他的评估结果，dynamic ntk算法效果最优，如下图所示：
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202308171941789.png)
上述所有的线性内插法和动态内插法，都已经在开源的transformers[13]，llama.cpp[14]等项目中落地应用了，感兴趣的同学可以参考具体实现的源代码。
在7月31日的苏剑林博客[16]中，作者按照$\beta$进制编码理论，进一步推导出一种据称更优的位置编码算法NTK-RoPE-mixed，感兴趣可以进一步阅读了解下。
P.S. 最新发布的大模型都已经内置NTK等内插算法，普遍支持8K到16K的上下文长度，本文所讲的内容实际上都已经过时。
