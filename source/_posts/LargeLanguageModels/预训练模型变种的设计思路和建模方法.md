---
title: 预训练模型变种的设计思路和建模方法
date: 2023-09-04 23:42:47
---


## 一. XLNet
1. 使用自回归语言模型结构，同时避免自编码模型中引入人造标记[mask]的问题
2. 使用双向上下文，不像传统自回归语言模型只能用单向的历史信息
3. 使用Transformer-XL作为主体框架，拥有更好的性能
#### 双流注意力
1. 内容表示
    1. 原始的transformer表示方法， 可以同时建模单词$x_i$及其上下文
2. 查询表示
    1. 能建模上下文信息$x_{z_{i} : i - 1}$以及目标位置$z_i$, 但不能看到单词$x_i$
  
  
## RoBERTa
1. 动态掩码
2. 舍弃NSP任务
3. 其他优化
    1. 更多的预训练数据
    2. 更大的批次以及更长的预训练步数
    3. 更大的词表


## ALBERT
1. 词向量因式分解
    1. Transformer隐含层维度H要远大于词向量维度E
    2. 词向量矩阵参数量降低至原来的1/8
2. 跨层参数共享
3. 句子顺序预测


## ELECTRA
1. 生成器，在[mask]的位置原来的词
2. 判别器， 判断输入句子每个词是否被替换 （判别器用于下游任务）

---
# 长文本建模模型

## transformer-XL
1. 状态复用的块界别循环
    1. 最大依赖长度 和n 和层数L呈线性增长。$\mathcal{O}(nL)$
2. 相对位置编码
    1. 第i个词和第j个词的注意力值$a_{i, j}$为：
        基于内容的相关度+内容相关的位置偏置+全局内容偏置+全局位置偏置
        

## Reformer
1. Q，K共享，减少注意力计算
2. 局部敏感哈希LSH
3. 可逆transformer。 降低内存占用空间。 时间换空间
```python
import torch
import torch.nn.functional as F

def lsh(x, num_hashes, num_buckets):
    # x: 输入张量，形状为(batch_size, sequence_length, hidden_size)
    # num_hashes: 要使用的哈希函数数量
    # num_buckets: 要哈希到的桶数
    
    # 计算随机投影向量
    proj = torch.randn(num_hashes, x.shape[-1], requires_grad=False)
    
    # 计算哈希码
    codes = F.linear(x, proj).sign()
    
    # 哈希到桶中
    buckets = codes @ torch.arange(num_hashes, device=codes.device).long().pow(2*num_hashes-1).flip(0)
    buckets = buckets % num_buckets
    
    return buckets
```


### longformer
1. 稀疏注意力
    1. 窗口滑动注意力
    2. 扩张滑动窗口注意力
    3. 全局注意力




# 生成模型
## BART
相对transformer的区别
1. 使用GeLU代替ReLUctant
2. 参数根据正态分布进行初始化
训练任务
1. 通过对含有噪声的输入文本进行去噪重构进行预训练


## UniLM
核心思想是通过使用不同的自注意力掩码矩阵控制每个词的注意力范围。从而实现不同的语言模型对于信息流的控制

## T5
将不同形式的任务统一转化为条件式生成任务。就需要一个统一的文本到文本的生成模型，就可以使用同样的训练方法预解码过程完成不同的自然语言处理任务。