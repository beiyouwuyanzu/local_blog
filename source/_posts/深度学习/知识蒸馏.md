---
title: 知识蒸馏（Knowledge Distillation）
date: 2023-05-08 21:04:50
mathjax: true
---

# 知识蒸馏（Knowledge Distillation）
## 概述
知识蒸馏（Knowledge Distillation）是一种将大型深度神经网络（Teacher Network）的知识压缩到小型深度神经网络（Student Network）中的方法。它的目的是在保持模型精度的同时，减少模型的大小和计算量，以适应移动设备和嵌入式设备等计算资源有限的场景。[[1]](https://cloud.tencent.com/developer/article/1814300)

知识蒸馏的实现原理是将原始的大型深度神经网络的输出结果和Softmax层的温度参数作为标签，来训练一个小型深度神经网络。[[4]](https://www.zhihu.com/question/366593202) 具体而言，知识蒸馏方法是通过最小化教师网络和学生网络在输出、特征、输入、网络参数等方面的不同，使学生网络可以模拟逼近教师网络的行为。[[4]](https://www.zhihu.com/question/366593202)


## 过程
知识蒸馏的关键在于如何将教师网络的知识蒸馏到学生网络中。目前，已经提出了多种不同的知识蒸馏方法，包括模型蒸馏、特征蒸馏、输出蒸馏和关系蒸馏等。其中，特征蒸馏是一种将教师网络和学生网络在特征层面进行蒸馏的方法，可以通过最小化教师网络和学生网络在特征层面的不同，来使学生网络可以模拟逼近教师网络的行为。

特征蒸馏的实现可以使用以下公式：[[1]](https://arxiv.org/abs/1912.13179)
$$L_{KD}(W_{s}) = H(y_{true}, P_{S}) + \lambda H(P_{T}^{\tau}, P_{S}^{\tau})$$


其中， $H$ 指交叉熵损失函数； $\lambda$是一个可调整参数，以平衡两个交叉熵；第一部分为Student的输出与Ground-truth的交叉熵损失；第二部分为Student与Teacher的softmax输出的交叉熵损失。具体而言，第二部分是通过在softmax层的输出中引入温度参数 $\tau$，来实现对输出分布的平滑化，从而使得模型更加鲁棒。

下面是一个知识蒸馏中加了温度这个变量之后的softmax函数公式：

$$q_i = \frac{exp(z_i/T)}{\sum_j exp(z_j/T)}$$

其中 $q_i$ 是每个类别输出的概率， $z_i$ 是每个类别输出的 logits， $T$ 就是温度。当温度 $T=1$ 时，这就是标准的 Softmax 公式。


## 优点
知识蒸馏的优点在于：

-   模型压缩：通过知识蒸馏可以将大型的深度神经网络压缩到小型的深度神经网络中，从而减小模型的大小和计算量，提高模型的运行速度。[[1]](https://cloud.tencent.com/developer/article/1814300)
-   模型泛化：知识蒸馏可以提高模型的泛化能力，使得模型具有更好的泛化性能，能够适应更广泛的数据分布。[[3]](https://blog.csdn.net/qq_42200733/article/details/130495370)
-   模型迁移：知识蒸馏可以将一个大型深度神经网络的知识迁移到一个小型深度神经网络中，从而实现模型迁移，提高模型的适应性。[[4]](https://www.zhihu.com/question/366593202)

## 缺点
知识蒸馏的缺点在于：

-   模型精度：知识蒸馏会损失一部分模型精度，因为压缩后的模型无法完全保留原始模型的所有信息。[[1]](https://cloud.tencent.com/developer/article/1814300)
-   计算复杂度：知识蒸馏需要训练两个模型，因此计算复杂度较大。[[3]](https://blog.csdn.net/qq_42200733/article/details/130495370)



## 代码实现

首先，需要导入必要的库和定义一些超参数。其中，T是温度参数，alpha是蒸馏损失函数中的权重系数。
```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision.datasets import CIFAR10
from torchvision import models
from pytorch_lightning.core.lightning import LightningModule
from pytorch_lightning import Trainer

T = 5
alpha = 0.5
batch_size = 128
learning_rate = 0.001
max_epochs = 100
```
然后，需要定义教师模型和学生模型。在本示例中，教师模型和学生模型都是基于ResNet18。需要注意的是，在学生模型中需要添加一个softmax层来输出概率分布。
```python
class TeacherModel(nn.Module):
    def __init__(self):
        super(TeacherModel, self).__init__()
        self.resnet = models.resnet18(pretrained=True)
        self.fc = nn.Linear(1000, 10)

    def forward(self, x):
        x = self.resnet(x)
        x = self.fc(x)
        return x

class StudentModel(nn.Module):
    def __init__(self):
        super(StudentModel, self).__init__()
        self.resnet = models.resnet18(pretrained=False)
        self.fc = nn.Linear(512, 10)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.resnet(x)
        x = self.fc(x)
        x = x / T
        x = self.softmax(x)
        return x
```
接下来，需要定义蒸馏损失函数。在本示例中，使用交叉熵损失函数作为蒸馏损失函数。其中，y_pred_teacher和y_pred_student分别表示教师模型和学生模型的预测结果。
```python
def distillation_loss(y_pred_teacher, y_pred_student, y_true):
    soft_teacher = nn.functional.softmax(y_pred_teacher / T, dim=1)
    soft_student = nn.functional.softmax(y_pred_student / T, dim=1)
    loss = alpha * nn.functional.kl_div(torch.log(soft_teacher), soft_student, reduction='batchmean') + \
           (1 - alpha) * nn.functional.cross_entropy(y_pred_student, y_true)
    return loss
```

> kl_div
> `nn.functional.kl_div`是PyTorch中用于计算KL散度的函数。KL散度是衡量两个概率分布之间差异的一种方法。在知识蒸馏中，KL散度被用来衡量学生模型的预测结果与教师模型的预测结果之间的差异。具体来说，KL散度衡量了学生模型的预测结果与教师模型的预测结果之间的信息差异，其中教师模型的预测结果被认为是更为准确的。因此，通过最小化KL散度，可以使学生模型更好地学习教师模型的知识
 -   KL散度公式：
	 $D_{KL}(p||q) = -\sum_{i} p_i log \frac{p_i}{q_i}$

	    -   其中，$p$和$q$分别表示两个概率分布
	    -   KL散度表示在真实分布$p$下，使用基于$q$的分布来表示所需信息的增益，或者说是使用$q$代替$p$时产生的信息损失。
	    -   KL散度不具有对称性，即$D_{KL}(p||q)$ ≠$D_{KL}(q||p)$
	    -   KL散度的值越小，表示两个分布越相似；越大，表示差异越大。
	    -   KL散度的值始终为非负数。
- 交叉熵公式:
$H(P,Q) = -\sum_{x} P(x)logQ(x)$
	-   交叉熵表示使用基于$q$的分布来表示真实分布$p$所需的平均比特数，或者说是使用$q$代替$p$时产生的平均信息损失。
	-   交叉熵具有对称性，即$H(P,Q) = H(Q,P)$
	- 交叉熵的值越小，表示两个分布越相似；越大，表示差异越大。
	- 交叉熵的值始终为非负数。