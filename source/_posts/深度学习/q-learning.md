---
title: q-learning
date: 2023-07-08 21:04:50
mathjax: true
---

Q-learning是一种经典的强化学习算法，用于学习最优策略的值函数。以下是Q-learning的基本流程：

1. 初始化：初始化状态值函数Q(s, a)和其他相关的参数，如学习率、折扣因子等。

2. 选择动作：根据当前状态$s$，使用某种策略（如$ε-greedy$策略）选择一个动作$a$。$ε-greedy$策略以ε的概率选择随机动作，以$1-ε$的概率选择具有最大Q值的动作。

3. 执行动作并观察环境：执行选择的动作$a$，与环境进行交互，观察新状态$s'$和奖励$r$。

4. 更新值函数：使用Bellman方程更新状态动作值函数Q(s, a)。根据Bellman方程，将当前状态动作对的估计值Q(s, a)更新为当前奖励r加上下一状态的最大动作值对应的Q值的乘积，加上一个折扣因子γ，再加上之前的估计值的一部分。

   $Q(s, a) = Q(s, a) + α * (r + γ * max Q(s', a') - Q(s, a))$
   
   其中，$α$是学习率，$γ$是折扣因子，$max Q(s', a')$表示在新状态$s'$下所有可能的动作中具有最大$Q$值的动作。

5. 更新状态和重复步骤2-4：将当前状态更新为新状态$s'$，重复执行步骤2至步骤4，选择动作、执行动作、观察奖励和新状态，并更新值函数，直到达到停止条件（如达到最大训练轮数、达到目标回报等）。

Q-learning的基本思想是通过不断更新值函数来逼近最优策略的值函数，从而得到最优策略。通过迭代的方式，Q-learning能够收敛到最优的值函数和最优的策略。


## 过程
Q-learning是一种基于值迭代的强化学习算法，它的流程如下：

1. 初始化状态值函数$Q(s,a)$。
2. 在每个时间步$t$，根据当前状态$s_t$选择一个动作$a_t$，并执行该动作。
3. 观察执行动作$a_t$后得到的奖励$r_{t+1}$和下一个状态$s_{t+1}$。
4. 根据下一个状态$s_{t+1}$和当前状态值函数$Q(s,a)$计算下一个状态的最大价值$V(s_{t+1})=\max_a Q(s_{t+1},a)$。
5. 根据贝尔曼方程更新当前状态值函数$Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha(r_{t+1} + \gamma V(s_{t+1}) - Q(s_t,a_t))$。
6. 重复步骤2-5，直到收敛。


## 为什么q-learning会收敛
在Q-learning算法中，每次选择的动作是基于当前状态值函数$Q(s,a)$的$\epsilon$-贪心策略，即以$1-\epsilon$的概率选择当前状态下最优的动作，以$\epsilon$的概率随机选择一个动作。在初始阶段，由于状态值函数$Q(s,a)$的不准确性，随机选择动作可以帮助算法探索更多的状态和动作，从而更好地优化策略。随着算法的迭代，状态值函数$Q(s,a)$会逐渐收敛到真实值，此时算法会逐渐减小$\epsilon$的值，使得选择最优动作的概率越来越大。因此，Q-learning算法可以逐步收敛到最优策略。
