---
title: 激活函数
date: 2023-05-08 21:04:50
mathjax: true
---


## 总览
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141653102.png)
----
> 以下内容摘自[paddledoc](https://paddlepedia.readthedocs.io/en/latest/tutorials/deep_learning/activation_functions/Activation_Function.html)
## 1. sigmoid
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141656564.png)
优点：

-  函数的输出映射在 (0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层；

- 求导容易；

缺点：

- 由于其软饱和性，一旦落入饱和区梯度就会接近于0，根据反向传播的链式法则，容易产生梯度消失，导致训练出现问题；

- Sigmoid函数的输出恒大于0。非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢；

- 计算时，由于具有幂运算，计算复杂度较高，运算速度较慢。

## 2. tanh
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141659373.png)

优点：

- 𝑡𝑎𝑛ℎ比 𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数收敛速度更快；

- 相比 𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数，𝑡𝑎𝑛ℎ是以 0为中心的；

缺点：

- 与 𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数相同，由于饱和性容易产生的梯度消失；

- 与𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数相同，由于具有幂运算，计算复杂度较高，运算速度较慢。


## 3. ReLU

![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141700048.png)

优点：

- 收敛速度快；

- 相较于 𝑠𝑖𝑔𝑚𝑜𝑖𝑑和 𝑡𝑎𝑛ℎ中涉及了幂运算，导致计算复杂度高ReLU可以更加简单的实现；

- 当输入 𝑥>=0时，ReLU 的导数为常数，这样可有效缓解梯度消失问题；

- 当 𝑥<0时，ReLU​ 的梯度总是 0，提供了神经网络的稀疏表达能力；

缺点：

- ReLU​ 的输出不是以 0为中心的；

- 神经元坏死现象，某些神经元可能永远不会被激活，导致相应参数永远不会被更新；

- 不能避免梯度爆炸问题；

## 4. LReLU
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141702809.png)

优点：

- 避免梯度消失；

- 由于导数总是不为零，因此可减少死神经元的出现；

缺点：

- LReLU​ 表现并不一定比 ReLU​ 好；

- 无法避免梯度爆炸问题；

## 5. PReLU
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141703232.png)

优点：

- PReLU​ 是 LReLU 的改进，可以自适应地从数据中学习参数；

- 收敛速度快、错误率低；

- PReLU 可以用于反向传播的训练，可以与其他层同时优化；

## 6. RReLU
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141704734.png)

优点：为负值输入添加了一个线性项，这个线性项的斜率在每一个节点上都是随机分配的（通常服从均匀分布）。

## 7. ELU
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141705343.png)

优点：

- 导数收敛为零，从而提高学习效率；

- 能得到负值输出，这能帮助网络向正确的方向推动权重和偏置变化；

- 防止死神经元出现。

缺点：

- 计算量大，其表现并不一定比 ReLU 好；

- 无法避免梯度爆炸问题；

## 8. SELU
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141706935.png)

优点：

- SELU 是 ELU 的一个变种。其中 λ 和 α 是固定数值（分别为 1.0507和 1.6726）;

- 经过该激活函数后使得样本分布自动归一化到 0均值和单位方差;

- 不会出现梯度消失或爆炸问题;

# 9. softsign
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141707210.png)

优点：

- 𝑠𝑜𝑓𝑡𝑠𝑖𝑔𝑛是 𝑡𝑎𝑛ℎ激活函数的另一个替代选择；

- 𝑠𝑜𝑓𝑡𝑠𝑖𝑔𝑛是反对称、去中心、可微分，并返回 −1和 1之间的值；

- 𝑠𝑜𝑓𝑡𝑠𝑖𝑔𝑛更平坦的曲线与更慢的下降导数表明它可以更高效地学习；

缺点：

- 导数的计算比𝑡𝑎𝑛ℎ更麻烦；


## 10. softplus
![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141709106.png)

优点：

- 作为 𝑟𝑒𝑙𝑢的一个不错的替代选择，𝑠𝑜𝑓𝑡𝑝𝑙𝑢𝑠能够返回任何大于 0的值。

- 与 𝑟𝑒𝑙𝑢不同，𝑠𝑜𝑓𝑡𝑝𝑙𝑢𝑠的导数是连续的、非零的，无处不在，从而防止出现死神经元。

缺点：

- 导数常常小于 1，也可能出现梯度消失的问题。
- 𝑠𝑜𝑓𝑡𝑝𝑙𝑢𝑠另一个不同于 𝑟𝑒𝑙𝑢的地方在于其不对称性，不以零为中心，可能会妨碍学习。



## 11. swish

![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141711270.png)


![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141712929.png)


![](https://raw.githubusercontent.com/dijiatrustlight/Chart_bed/master/img/202305141712358.png)

优点：

- 当 𝑥>0时，不存在梯度消失的情况；当 𝑥<0时，神经元也不会像 ReLU 一样出现死亡的情况；

- 𝑠𝑤𝑖𝑠ℎ处处可导，连续光滑；

- 𝑠𝑤𝑖𝑠ℎ并非一个单调的函数；

- 提升了模型的性能；

缺点：

- 计算量大；


## 激活函数的选择
- 浅层网络在分类器时，𝑠𝑖𝑔𝑚𝑜𝑖𝑑函数及其组合通常效果更好。

- 由于梯度消失问题，有时要避免使用 𝑠𝑖𝑔𝑚𝑜𝑖𝑑和 𝑡𝑎𝑛ℎ函数。

- 𝑟𝑒𝑙𝑢函数是一个通用的激活函数，目前在大多数情况下使用。

- 如果神经网络中出现死神经元，那么 𝑝𝑟𝑒𝑙𝑢函数就是最好的选择。

- 𝑟𝑒𝑙𝑢函数只能在隐藏层中使用。

- 通常，可以从 𝑟𝑒𝑙𝑢函数开始，如果 𝑟𝑒𝑙𝑢函数没有提供最优结果，再尝试其他激活函数。