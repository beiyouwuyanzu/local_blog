---
title: 强化学习ppo
date: 2023-07-08 21:04:50
mathjax: true
---


## ppo具体流程

1. 初始化：初始化策略网络参数。

2. 数据收集：使用当前策略与环境进行交互，收集一定数量的轨迹数据。

3. 计算优势估计：使用收集到的轨迹数据，计算每个状态的优势估计，通常使用基于价值函数的方法，如广义优势估计（Generalized Advantage Estimation，GAE）。

4. 计算旧策略概率比例：使用收集到的轨迹数据，计算每个状态下旧策略和新策略之间的概率比例，用于后续的策略更新。

5. 更新策略网络：使用收集到的轨迹数据和计算得到的优势估计，对策略网络进行更新。更新过程通常使用随机梯度上升（Stochastic Gradient Ascent）的方法，最大化目标函数。

6. 梯度裁剪：对策略更新的梯度进行裁剪，以限制更新的幅度，避免过大的策略变动。

7. 多次迭代：重复执行步骤2至步骤6，进行多次迭代的策略更新，以进一步优化策略模型。

8. 输出策略模型：训练完成后，输出训练得到的策略模型，用于在真实环境中进行决策。

## ppo算法里怎么计算优势估计

在PPO算法中，可以使用广义优势估计（Generalized Advantage Estimation，GAE）来计算状态的优势估计。GAE是一种常用的方法，用于估计状态的优势值，以便在策略更新时提供更准确的梯度信号。以下是计算优势估计的一般步骤：

1. 收集轨迹数据：使用当前策略与环境进行交互，收集一定数量的轨迹数据，包括状态、动作、奖励等信息。

2. 计算累积奖励：对于每个轨迹，从后向前计算累积奖励。从最后一个状态开始，使用折扣因子（通常表示为γ，介于0和1之间）乘以后续状态的奖励，并将其累积到当前状态的奖励中。计算公式如下：

   $R_t = r_t + γ * R_{t+1}$

   其中，$R_t$表示在时刻t的累积奖励，$r_t$表示时刻t的即时奖励。

3. 计算TD误差：计算每个状态的TD（Temporal Difference）误差，用于估计状态值函数的误差。TD误差可以通过计算当前奖励和下一个状态的预测值之间的差异来获得。计算公式如下：

   $δ_t = r_t + γ * V(s_{t+1}) - V(s_t)$

   其中，$δ_t$表示在时刻t的TD误差，$s_t$和$s_{t+1}$分别表示时刻t和t+1的状态，$V(s)$ 表示状态值函数的预测值。

4. 计算优势估计：使用TD误差来计算优势估计。可以使用GAE来计算优势估计，其中一个参数λ（介于0和1之间）用于控制前后状态间的时间关联性。计算公式如下：

  $A_t^{GAE(λ)} = ∑_{i=0}^{T-t-1} (γ * λ)^i * δ_{t+i}$

   其中，$A_t^{GAE(λ)}$表示在时刻t的广义优势估计，T表示轨迹的长度。

   在实际计算中，还可以对GAE进行标准化处理，以减小估计误差的影响。

通过计算优势估计，可以为PPO算法提供更准确的梯度信号，从而改善策略的更新效果。

## 为什么actor network和 critic network 要一起训练, 两个分开单独训练不好吗
[1]: https://borrowastep.net/p/ppo-actor-critic--1w8dmrkf7 "PPO 和 Actor-Critic 比较 | Borrow A Step"
[2]: https://zhuanlan.zhihu.com/p/111049450 "如何直观理解PPO算法?[理论篇] - 知乎 - 知乎专栏"
[3]: https://zhuanlan.zhihu.com/p/624914233 "强化学习8-Actor-critic PPO-代码详细注释 - 知乎 - 知乎专栏"
[4]: https://blog.csdn.net/weixin_43145941/article/details/115049184 "浅析强化学习Proximal Policy Optimization Algorithms (PPO)"

PPO算法中，actor network和critic network一起训练的原因是，actor network的更新依赖于critic network的输出，而critic network的输出又依赖于actor network的输出。因此，两个网络需要同时训练，以便更好地优化策略[^1^][1]。


## critic network 和 advantage function 是什么关系

1. Critic网络：Critic网络在强化学习中用于估计状态值或动作值函数。它可以通过值函数估计来评估策略的好坏。Critic网络的输出可以是状态值函数（V值函数），表示在某个状态下的长期累积回报；或者是动作值函数（Q值函数），表示在某个状态下采取某个动作后的长期累积回报。

2. 优势函数：优势函数是一种用于衡量某个状态下采取某个动作相对于平均策略的优势程度的函数。它表示采取某个动作相对于当前策略所带来的额外收益。优势函数可以通过从Critic网络中减去状态值函数来计算得到，即 Adv(s, a) = Q(s, a) - V(s)，其中 s 表示状态，a 表示动作。

因此，Critic网络提供了对策略的估值，而优势函数利用了Critic网络的输出来衡量策略的优劣。优势函数的正负值表示了某个动作相对于当前策略的优势或劣势。在训练过程中，优势函数常常被用于计算策略梯度，以指导策略的优化。通过最大化优势函数，可以使得策略朝着产生更高回报的方向改进。

总结起来，Critic网络提供了对策略的估值，优势函数利用了Critic网络的输出来衡量策略的优劣，进而指导策略的优化。这两者在强化学习中相互关联，共同作用于策略的改进和学习过程。


## 状态值函数（V值函数）表示在某个状态下的长期累积回报，可以通过以下几种方法来计算：

1. 蒙特卡洛方法：使用蒙特卡洛方法可以通过采样多个完整的轨迹（从起始状态到终止状态）来估计状态值函数。在每个轨迹中，累积所有访问过的状态的回报，并计算其平均值作为该状态的估计值。重复多次采样和计算平均值可以得到更准确的估计。

2. 时序差分学习（Temporal Difference, TD）方法：TD方法是一种基于单步更新的方法，通过在每个时间步更新估计的状态值函数。常用的TD方法包括TD(0)、SARSA和Q-learning等。TD方法通过将下一个状态的估计值与当前状态的奖励结合起来，以更新当前状态的估计值。通过迭代地更新状态值函数，可以逐渐收敛到真实的状态值。

3. 基于函数逼近的方法：如果状态空间较大，使用表格存储状态值函数变得不可行。此时可以使用函数逼近方法，如线性函数逼近、神经网络等来估计状态值函数。通过将状态作为输入，使用函数逼近器（如神经网络）的输出作为状态值函数的估计值，通过训练函数逼近器来逐步优化状态值的估计。

## 计算状态值TD(0)方法
状态值函数的计算方法有很多种，其中最常用的是TD(0)方法。TD(0)方法是一种基于时间差分的方法，它通过比较当前状态的估计值和下一个状态的真实值来更新状态值函数。具体来说，TD(0)方法的更新公式如下：

$$V(s_t) \leftarrow V(s_t) + \alpha(r_{t+1} + \gamma V(s_{t+1}) - V(s_t))$$

其中，$V(s_t)$表示在状态$s_t$下的状态值函数，$\alpha$表示学习率，$r_{t+1}$表示在状态$s_t$下执行动作$a_t$后得到的奖励，$\gamma$表示折扣因子，$s_{t+1}$表示在执行动作$a_t$后进入的下一个状态。
