---
title: 常见网络的代码实现
date: 2023-05-08 21:04:50
mathjax: true
---


### 全连接
以下是一个使用numpy实现的全连接神经网络的Python代码，其中包括了前向传播和反向传播的实现。代码中的神经网络包含了两个隐藏层和一个输出层，其中每个隐藏层包含了256个神经元，输出层包含了10个神经元，用于识别手写数字。
```python
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros(hidden_size)

        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros(output_size)

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def softmax(self, x):
        exps = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exps / np.sum(exps, axis=1, keepdims=True)

    def forward(self, X):
        Z1 = np.dot(X, self.W1) + self.b1
        A1 = self.sigmoid(Z1)

        Z2 = np.dot(A1, self.W2) + self.b2
        A2 = self.softmax(Z2)

        return A1, A2

    def backward(self, X, y_true, A1, A2):
        m = X.shape[0]

        dZ2 = A2 - y_true
        dW2 = np.dot(A1.T, dZ2) / m
        db2 = np.sum(dZ2, axis=0) / m

        dA1 = np.dot(dZ2, self.W2.T)
        dZ1 = dA1 * A1 * (1 - A1)
        dW1 = np.dot(X.T, dZ1) / m
        db1 = np.sum(dZ1, axis=0) / m

        return dW1, db1, dW2, db2



input_size = 784
hidden_size = 32
output_size = 10

X = np.random.randn(100, input_size)
y = np.random.randint(output_size, size=100)
y_true = np.eye(output_size)[y]

nn = NeuralNetwork(input_size, hidden_size, output_size)

A1, A2 = nn.forward(X)

dW1, db1, dW2, db2 = nn.backward(X, y_true, A1, A2)

```

### 矩阵求导注意点
求导的参数在dot()的什么位置, delta(左值)就在什么位置
```python
Z1 = np.dot(X, self.W1) + self.b1
```
$d_{w1}$=$\frac{d_{z1}}{d_x}$ = np.dot(x.T, dz1)
```python
Z2 = np.dot(A1, self.W2) + self.b2
```
$d_{a1}$=$\frac{d_{z2}}{d_{w2}}$ = np.dot(dz2, w2.T)

至于bias求到就是误差总和评分
`db2 = np.sum(dZ2, axis=0) / m`


### 卷积神经网络实现

具体来说，假设网络的输出为$y$，真实标签为$t$，则输出层的误差可以使用均方误差（Mean Square Error，MSE）函数来计算：

$$
E = \frac{1}{2}\sum_i(y_i-t_i)^2
$$

其中$i$表示输出层的神经元编号。然后，通过链式法则将误差逐层向前传递，计算每个权重和偏置的梯度。以卷积层为例，设输入为$x$，输出为$y$，卷积核为$w$，则卷积层的梯度计算如下：

$$
\frac{\partial E}{\partial w} = \frac{\partial E}{\partial y}\cdot \frac{\partial y}{\partial w}
$$

其中$\frac{\partial E}{\partial y}$表示输出误差对输出值的偏导数，可以通过前向传播计算得到；$\frac{\partial y}{\partial w}$表示输出值对卷积核的偏导数，可以通过卷积操作计算得到。最终，通过梯度下降算法更新权重和偏置。

```python
# 定义卷积函数
def convolve(X, W, b):
    h, w, c = X.shape
    kh, kw, kc, nc = W.shape
    Y = np.zeros((h-kh+1, w-kw+1, nc))
    for i in range(h-kh+1):
        for j in range(w-kw+1):
            # 对每个位置进行卷积操作
            for k in range(nc):
                Y[i,j,k] = np.sum(X[i:i+kh,j:j+kw,:]*W[:,:,:,k])+b[k]
    return Y


# 定义反向传播函数
def backward(X, Y, Z, O, t, W, V, b, c, lr):
    # 计算卷积层和全连接层的梯度
    dY = Y - t
    dO = dY.dot(V.T)
    dV = Z.T.dot(dY)
    dc = np.sum(dY, axis=0)
    dZ = dO
    dZ[Z<=0] = 0
    dW = np.zeros(W.shape)
    h, w, c = X.shape
    kh, kw, kc, nc = W.shape
    for i in range(h-kh+1):
        for j in range(w-kw+1):
            for k in range(nc):
                dW[:,:,:,k] += X[i:i+kh,j:j+kw,:]*dZ[i,j,k]
    db = np.sum(dZ, axis=(0,1))
    # 更新卷积层和全连接层的权重和偏置
    W -= lr*dW
    b -= lr*db
    V -= lr*dV
    c -= lr*dc
    return W, b, V, c

```


### 注意力实现
1. Scaled Dot-Product Attention
```python
import torch
import torch.nn.functional as F

class ScaledDotProductAttention(torch.nn.Module):
    def __init__(self, d_k):
        super(ScaledDotProductAttention, self).__init__()
        self.d_k = d_k

    def forward(self, q, k, v, mask=None):
        # 计算点积
        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))
        # 应用掩码
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        # softmax归一化
        p_attn = F.softmax(scores, dim=-1)
        # 加权求和
        attn_output = torch.matmul(p_attn, v)
        return attn_output, p_attn

```



2. Multi-Head Attention
```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super(MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        self.fc = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        batch_size = x.size(0)
        # Compute queries, keys, and values
        Q = self.query(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.key(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.value(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        # Compute attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k).float())
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        attn_weights = nn.functional.softmax(scores, dim=-1)
        # Apply attention weights to values
        attn_output = torch.matmul(attn_weights, V)
        # Merge attention heads
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        # Apply final linear layer
        output = self.fc(attn_output)
        return output

```
3. 传统seq2seq的注意力
```python
import torch
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super(Encoder, self).__init__()
        self.input_dim = input_dim
        self.emb_dim = emb_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        self.dropout = dropout
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)

    def forward(self, src):
        embedded = self.embedding(src)
        embedded = nn.functional.dropout(embedded, p=self.dropout, training=self.training)
        output, (hidden, cell) = self.rnn(embedded)
        return output, hidden, cell

class Attention(nn.Module):
    def __init__(self, enc_hid_dim, dec_hid_dim):
        super(Attention, self).__init__()
        self.enc_hid_dim = enc_hid_dim
        self.dec_hid_dim = dec_hid_dim
        self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)
        self.v = nn.Linear(dec_hid_dim, 1, bias=False)

    def forward(self, hidden, encoder_outputs):
        batch_size = encoder_outputs.shape[0]
        src_len = encoder_outputs.shape[1]
        repeated_hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
        energy = torch.tanh(self.attn(torch.cat((repeated_hidden, encoder_outputs), dim=2)))
        attention = self.v(energy).squeeze(2)
        return nn.functional.softmax(attention, dim=1)

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):
        super(Decoder, self).__init__()
        self.output_dim = output_dim
        self.emb_dim = emb_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        self.dropout = dropout
        self.attention = attention
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout)
        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)

    def forward(self, input, hidden, cell, encoder_outputs):
        input = input.unsqueeze(0)
        embedded = self.embedding(input)
        embedded = nn.functional.dropout(embedded, p=self.dropout, training=self.training)
        attn_weights = self.attention(hidden[-1], encoder_outputs)
        attn_weights = attn_weights.unsqueeze(1)
        weighted = torch.bmm(attn_weights, encoder_outputs)
        rnn_input = torch.cat((embedded, weighted), dim=2)
        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))
        output = torch.cat((embedded, hidden, weighted), dim=2)
        prediction = self.fc_out(output.squeeze(0))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        encoder_outputs, hidden, cell = self.encoder(src)
        input = trg[0,:]
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)
            outputs[t] = output
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[t] if teacher_force else top1
        return outputs


```